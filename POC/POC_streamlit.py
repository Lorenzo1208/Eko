import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
from prophet import Prophet
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Eko - Surveillance des Rivi√®res d'Occitanie",
    page_icon="üíß",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Fonction pour charger les donn√©es des mesures hydrom√©triques
@st.cache_data
def load_hydro_data(file_path="mesures_hydro_occitanie_2020_2025_20250520_100820.csv"):
    try:
        # V√©rifier si le fichier existe dans le r√©pertoire POC
        import os
        if not os.path.exists(file_path) and os.path.exists(f"POC/{file_path}"):
            file_path = f"POC/{file_path}"
        
        st.write(f"Tentative de chargement du fichier : {file_path}")
        
        # Chargement du fichier CSV avec des options plus robustes
        df = pd.read_csv(file_path, 
                         encoding='utf-8', 
                         low_memory=False, 
                         on_bad_lines='skip')
        
        # V√©rifier que les colonnes attendues existent
        date_column = None
        if 'date_obs_elab' in df.columns:
            date_column = 'date_obs_elab'
        elif 'date_obs' in df.columns:
            date_column = 'date_obs'
        else:
            st.warning("Aucune colonne de date trouv√©e. V√©rifiez le format du fichier.")
            # Cr√©er une colonne date par d√©faut
            df['date'] = pd.Timestamp('2025-01-01')
            return df
        
        # Convertir les dates en datetime avec gestion des erreurs
        df['date'] = pd.to_datetime(df[date_column], errors='coerce')
        
        # Supprimer les lignes avec des dates invalides
        invalid_dates = df['date'].isna().sum()
        if invalid_dates > 0:
            st.warning(f"{invalid_dates} lignes avec des dates invalides ont √©t√© trouv√©es et seront ignor√©es.")
            df = df.dropna(subset=['date'])
        
        # Extraire des informations temporelles suppl√©mentaires
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['day'] = df['date'].dt.day
        df['day_of_year'] = df['date'].dt.dayofyear
        
        # Assurez-vous que la colonne 'code_station_id' existe
        if 'code_station_id' not in df.columns and 'code_entite' in df.columns:
            df['code_station_id'] = df['code_entite']
        
        # Assurez-vous que la colonne 'nom_station' existe
        if 'nom_station' not in df.columns and 'libelle_station' in df.columns:
            df['nom_station'] = df['libelle_station']
        
        # V√©rifier que les colonnes essentielles existent
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in df.columns else 'resultat_obs'
        if flow_column not in df.columns:
            st.error(f"Colonne de d√©bit '{flow_column}' non trouv√©e dans le fichier.")
            # Cr√©er une colonne factice pour √©viter les erreurs
            df[flow_column] = 10.0  # Valeur par d√©faut
        
        st.success(f"Fichier charg√© avec succ√®s : {len(df)} lignes de donn√©es.")
        return df
    
    except FileNotFoundError:
        st.error(f"Fichier non trouv√© : {file_path}")
        st.info("Veuillez uploader votre fichier CSV de donn√©es hydrom√©triques.")
        return None
    except pd.errors.EmptyDataError:
        st.error(f"Le fichier {file_path} est vide.")
        return None
    except pd.errors.ParserError:
        st.error(f"Erreur lors de l'analyse du fichier {file_path}. Format CSV incorrect.")
        return None
    except Exception as e:
        import traceback
        st.error(f"Erreur lors du chargement des donn√©es: {str(e)}")
        st.code(traceback.format_exc())
        return None

# Fonction pour cr√©er un DataFrame des stations uniques avec leurs coordonn√©es
@st.cache_data
def extract_stations_info(data_df):
    if data_df is None:
        return None
    
    try:
        # S√©lectionner les colonnes pertinentes et supprimer les doublons
        station_cols = ['code_station_id', 'nom_station', 'longitude', 'latitude']
        
        # V√©rifier si toutes les colonnes existent
        if not all(col in data_df.columns for col in station_cols):
            # Pour les colonnes manquantes, cr√©er des mappings par d√©faut
            if 'code_station_id' not in data_df.columns and 'code_station' in data_df.columns:
                data_df['code_station_id'] = data_df['code_station']
                
            # Si des coordonn√©es sont manquantes, g√©n√©rer des coordonn√©es fictives centr√©es sur l'Occitanie
            if 'longitude' not in data_df.columns:
                data_df['longitude'] = 1.444 + np.random.normal(0, 0.5, len(data_df))
            if 'latitude' not in data_df.columns:
                data_df['latitude'] = 43.6 + np.random.normal(0, 0.3, len(data_df))
        
        # S√©lectionner les colonnes disponibles
        available_cols = [col for col in station_cols if col in data_df.columns]
        
        # Cr√©er le DataFrame des stations
        stations_df = data_df[available_cols].drop_duplicates().reset_index(drop=True)
        
        # Ajouter des informations sur le cours d'eau si disponibles
        if 'libelle_cours_eau' in data_df.columns:
            stations_df = pd.merge(
                stations_df,
                data_df[['code_station_id', 'libelle_cours_eau']].drop_duplicates(),
                on='code_station_id',
                how='left'
            )
        
        return stations_df
    except Exception as e:
        st.error(f"Erreur lors de l'extraction des informations des stations: {str(e)}")
        return None

# Fonction pour calculer les seuils statistiques par station
@st.cache_data
def calculate_thresholds(df):
    if df is None or len(df) == 0:
        return None
        
    try:
        # Identifier la colonne de d√©bit
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in df.columns else 'resultat_obs'
        
        # Grouper par station
        grouped = df.groupby('code_station_id')
        
        # Calculer les seuils
        thresholds = grouped.agg(
            debit_min=(flow_column, 'min'),
            debit_max=(flow_column, 'max'),
            debit_moyen=(flow_column, 'mean'),
            seuil_vigilance=(flow_column, lambda x: np.percentile(x, 75)),
            seuil_alerte=(flow_column, lambda x: np.percentile(x, 90)),
            seuil_alerte_renforcee=(flow_column, lambda x: np.percentile(x, 95)),
            seuil_crise=(flow_column, lambda x: np.percentile(x, 98))
        ).reset_index()
        
        return thresholds
    except Exception as e:
        st.error(f"Erreur lors du calcul des seuils: {str(e)}")
        return None

# Fonction pour calculer les seuils mensuels
@st.cache_data
def calculate_monthly_thresholds(df):
    if df is None or len(df) == 0:
        return None
        
    try:
        # Identifier la colonne de d√©bit
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in df.columns else 'resultat_obs'
        
        # Grouper par station et mois
        grouped = df.groupby(['code_station_id', 'month'])
        
        # Calculer les seuils
        thresholds = grouped.agg(
            debit_min=(flow_column, 'min'),
            debit_max=(flow_column, 'max'),
            debit_moyen=(flow_column, 'mean'),
            seuil_vigilance=(flow_column, lambda x: np.percentile(x, 75)),
            seuil_alerte=(flow_column, lambda x: np.percentile(x, 90)),
            seuil_alerte_renforcee=(flow_column, lambda x: np.percentile(x, 95)),
            seuil_crise=(flow_column, lambda x: np.percentile(x, 98))
        ).reset_index()
        
        return thresholds
    except Exception as e:
        st.error(f"Erreur lors du calcul des seuils mensuels: {str(e)}")
        return None

# Fonction pour cr√©er un mod√®le de pr√©diction (RandomForest ou Prophet)
@st.cache_resource
def create_prediction_model(df, station_id, method="randomforest"):
    if df is None or len(df) == 0:
        return None
        
    try:
        # Filtrer les donn√©es pour la station s√©lectionn√©e
        station_data = df[df['code_station_id'] == station_id].copy()  # Ajout de .copy()
        
        # V√©rifier s'il y a suffisamment de donn√©es
        if len(station_data) < 10:
            st.warning(f"Pas assez de donn√©es pour la station {station_id}. Le mod√®le peut √™tre impr√©cis.")
            if len(station_data) < 5:
                return None
        
        # Choisir le bon mod√®le en fonction de la m√©thode
        if method.lower() == "prophet":
            # Pr√©parer les donn√©es pour Prophet
            flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in station_data.columns else 'resultat_obs'
            prophet_df = pd.DataFrame({
                'ds': station_data['date'],
                'y': station_data[flow_column]
            })
            
            # Cr√©er et entra√Æner le mod√®le
            model = Prophet(daily_seasonality=True)
            model.fit(prophet_df)
            return {"type": "prophet", "model": model}
            
        else:  # RandomForest par d√©faut
            # Pr√©parer les donn√©es pour RandomForest
            flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in station_data.columns else 'resultat_obs'
            
            # Caract√©ristiques de base pour le mod√®le
            features = ['month', 'day_of_year']
            
            # Ajouter la saison encod√©e si disponible
            saison_encoder = None
            if 'saison' in station_data.columns:
                # Encoder la saison
                le = LabelEncoder()
                station_data.loc[:, 'saison_encoded'] = le.fit_transform(station_data['saison'])  # Utilisation de .loc
                features.append('saison_encoded')
                saison_encoder = le
            
            # S'assurer que toutes les features existent
            X = station_data[features].copy()
            y = station_data[flow_column]
            
            # Diviser en ensembles d'entra√Ænement et de test
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Entra√Æner le mod√®le
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rf_model.fit(X_train, y_train)
            
            # Stocker les informations importantes pour les pr√©dictions
            model_info = {
                "type": "randomforest",
                "model": rf_model,
                "features": features,
                "saison_encoder": saison_encoder,
                "has_saison_feature": 'saison' in station_data.columns
            }
            
            return model_info
            
    except Exception as e:
        st.error(f"Erreur lors de la cr√©ation du mod√®le de pr√©diction: {str(e)}")
        return None

# Fonction pour faire des pr√©dictions avec le mod√®le
def make_predictions(model_info, df, station_id, days=30):
    if model_info is None:
        return None
        
    try:
        # R√©cup√©rer les informations de la station
        station_data = df[df['code_station_id'] == station_id]
        
        # D√©terminer la derni√®re date disponible
        last_date = station_data['date'].max()
        
        # G√©n√©rer les dates futures
        future_dates = [last_date + timedelta(days=i) for i in range(1, days + 1)]
        
        # Faire les pr√©dictions en fonction du type de mod√®le
        if model_info["type"] == "prophet":
            # Cr√©er un DataFrame avec les dates futures pour Prophet
            future = pd.DataFrame({"ds": pd.date_range(start=last_date, periods=days+1)[1:]})
            
            # Faire les pr√©dictions
            forecast = model_info["model"].predict(future)
            
            # Pr√©parer le r√©sultat
            result = pd.DataFrame({
                "date": forecast["ds"],
                "prediction": forecast["yhat"],
                "lower_bound": forecast["yhat_lower"],
                "upper_bound": forecast["yhat_upper"]
            })
            
        else:  # RandomForest
            # Cr√©er les caract√©ristiques pour les dates futures
            future_features = []
            for date in future_dates:
                features_dict = {
                    "month": date.month,
                    "day_of_year": date.timetuple().tm_yday
                }
                
                # Ajouter la saison si n√©cessaire
                if model_info.get("has_saison_feature", False) and model_info.get("saison_encoder") is not None:
                    # D√©terminer la saison en fonction du mois
                    if 3 <= date.month <= 5:
                        saison = "Printemps"
                    elif 6 <= date.month <= 8:
                        saison = "√ât√©"
                    elif 9 <= date.month <= 11:
                        saison = "Automne"
                    else:
                        saison = "Hiver"
                    
                    features_dict["saison_encoded"] = model_info["saison_encoder"].transform([saison])[0]
                    
                future_features.append(features_dict)
            
            # Cr√©er un DataFrame avec les caract√©ristiques futures
            future_df = pd.DataFrame(future_features)
            
            # S'assurer que toutes les features utilis√©es lors de l'entra√Ænement sont pr√©sentes
            for feature in model_info["features"]:
                if feature not in future_df.columns:
                    st.warning(f"Feature manquante: {feature}. Le mod√®le pourrait √™tre impr√©cis.")
                    # Si c'est la saison qui manque, on pourrait cr√©er une colonne factice
                    if feature == 'saison_encoded' and 'month' in future_df.columns:
                        # Cr√©er une saison encod√©e bas√©e sur le mois
                        future_df['saison_encoded'] = future_df['month'].apply(
                            lambda m: 0 if m in [12, 1, 2] else  # Hiver
                                    1 if m in [3, 4, 5] else     # Printemps
                                    2 if m in [6, 7, 8] else     # √ât√©
                                    3                            # Automne
                        )
            
            # Faire les pr√©dictions en utilisant uniquement les features disponibles dans le mod√®le
            available_features = [f for f in model_info["features"] if f in future_df.columns]
            predictions = model_info["model"].predict(future_df[available_features])
            
            # Calculer des intervalles de confiance approximatifs
            std_dev = np.std(predictions) if len(predictions) > 1 else 0
            lower_bounds = predictions - 1.96 * std_dev
            upper_bounds = predictions + 1.96 * std_dev
            
            # Pr√©parer le r√©sultat
            result = pd.DataFrame({
                "date": future_dates,
                "prediction": predictions,
                "lower_bound": lower_bounds,
                "upper_bound": upper_bounds
            })
            
        return result
    except Exception as e:
        st.error(f"Erreur lors de la pr√©diction: {str(e)}")
        st.exception(e)  # Afficher la trace compl√®te pour d√©boguer
        return None

# Fonction pour √©valuer la performance du mod√®le
def evaluate_model(model_info, df, station_id):
    """√âvalue la performance du mod√®le en utilisant une validation temporelle"""
    if model_info is None or df is None:
        return None
    
    try:
        # Filtrer les donn√©es pour la station s√©lectionn√©e
        station_data = df[df['code_station_id'] == station_id].copy()
        
        # Trier les donn√©es par date
        station_data = station_data.sort_values('date')
        
        # S'il y a moins de 10 points de donn√©es, impossible d'√©valuer correctement
        if len(station_data) < 10:
            return {
                "error": "Donn√©es insuffisantes pour l'√©valuation du mod√®le",
                "metrics": None,
                "predictions": None,
                "actuals": None,
                "dates": None
            }
            
        # Identifier la colonne de d√©bit
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in station_data.columns else 'resultat_obs'
        
        # Diviser les donn√©es: 80% pour l'entra√Ænement, 20% pour le test (validation temporelle)
        split_idx = int(len(station_data) * 0.8)
        train_data = station_data.iloc[:split_idx].copy()
        test_data = station_data.iloc[split_idx:].copy()
        
        # S'il n'y a pas de donn√©es de test, impossible d'√©valuer
        if len(test_data) == 0:
            return {
                "error": "P√©riode de test vide, impossible d'√©valuer le mod√®le",
                "metrics": None,
                "predictions": None,
                "actuals": None,
                "dates": None
            }
            
        # Entra√Æner le mod√®le sur les donn√©es d'entra√Ænement
        if model_info["type"] == "prophet":
            # Pr√©parer les donn√©es pour Prophet
            prophet_df = pd.DataFrame({
                'ds': train_data['date'],
                'y': train_data[flow_column]
            })
            
            # Cr√©er et entra√Æner le mod√®le
            model = Prophet(daily_seasonality=True)
            model.fit(prophet_df)
            
            # Faire des pr√©dictions sur les dates de test
            future = pd.DataFrame({"ds": test_data['date']})
            forecast = model.predict(future)
            
            # Extraire les pr√©dictions
            predictions = forecast["yhat"].values
            
        else:  # RandomForest
            # Pr√©parer les caract√©ristiques pour RandomForest
            features = ['month', 'day_of_year']
            
            # Ajouter la saison encod√©e si disponible
            if 'saison' in train_data.columns:
                le = LabelEncoder()
                train_data.loc[:, 'saison_encoded'] = le.fit_transform(train_data['saison'])
                features.append('saison_encoded')
                
                # Encoder √©galement les donn√©es de test
                test_data.loc[:, 'saison_encoded'] = le.transform(test_data['saison'])
            
            # Entra√Æner le mod√®le
            X_train = train_data[features].copy()
            y_train = train_data[flow_column]
            
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rf_model.fit(X_train, y_train)
            
            # Faire des pr√©dictions sur les donn√©es de test
            X_test = test_data[features].copy()
            predictions = rf_model.predict(X_test)
        
        # R√©cup√©rer les valeurs r√©elles
        actuals = test_data[flow_column].values
        
        # Calculer les m√©triques
        mae = mean_absolute_error(actuals, predictions)
        rmse = np.sqrt(mean_squared_error(actuals, predictions))
        r2 = r2_score(actuals, predictions)
        
        # Calculer MAPE (Mean Absolute Percentage Error)
        # √âviter la division par z√©ro
        mape = np.mean(np.abs((actuals - predictions) / np.maximum(np.ones(len(actuals)), np.abs(actuals)))) * 100
        
        metrics = {
            "MAE": mae,  # Erreur absolue moyenne
            "RMSE": rmse,  # Racine carr√©e de l'erreur quadratique moyenne
            "R¬≤": r2,  # Coefficient de d√©termination
            "MAPE": mape  # Pourcentage d'erreur absolue moyen
        }
        
        return {
            "error": None,
            "metrics": metrics,
            "predictions": predictions,
            "actuals": actuals,
            "dates": test_data['date'].values
        }
        
    except Exception as e:
        import traceback
        return {
            "error": f"Erreur lors de l'√©valuation du mod√®le: {str(e)}\n{traceback.format_exc()}",
            "metrics": None,
            "predictions": None,
            "actuals": None,
            "dates": None
        }

# Fonction pour afficher une carte des stations avec PyDeck
def display_station_map(stations_df, thresholds_df):
    if stations_df is None or len(stations_df) == 0:
        st.error("Aucune donn√©e de station disponible pour la carte.")
        return None
        
    try:
        # Fusionner les donn√©es des stations avec les seuils
        if thresholds_df is not None:
            map_data = pd.merge(
                stations_df, 
                thresholds_df, 
                left_on='code_station_id',
                right_on='code_station_id', 
                how='left'
            )
        else:
            map_data = stations_df.copy()
        
        # Afficher les valeurs num√©riques correctement format√©es
        for col in ['debit_moyen', 'seuil_alerte', 'seuil_crise']:
            if col in map_data.columns:
                map_data[f'{col}_fmt'] = map_data[col].apply(lambda x: f"{x:.2f}" if pd.notnull(x) else "N/A")
        
        # Cr√©er une colonne pour la couleur en fonction du seuil
        def determine_color(row):
            if 'debit_moyen' not in row or pd.isna(row.get('debit_moyen')):
                return [150, 150, 150]  # Gris pour les stations sans donn√©es
            
            if 'seuil_crise' in row and not pd.isna(row.get('seuil_crise')):
                if row['debit_moyen'] > row['seuil_crise']:
                    return [255, 0, 0]  # Rouge pour stations au-dessus du seuil de crise
                elif row['debit_moyen'] > row['seuil_alerte']:
                    return [255, 165, 0]  # Orange pour stations au-dessus du seuil d'alerte
                else:
                    return [0, 100, 255]  # Bleu pour stations normales
            else:
                return [0, 100, 255]  # Bleu par d√©faut
        
        map_data['color'] = map_data.apply(determine_color, axis=1)
        
        # Cr√©er la couche des stations - TAILLE R√âDUITE
        stations_layer = pdk.Layer(
            'ScatterplotLayer',
            data=map_data,
            get_position=['longitude', 'latitude'],
            get_color='color',
            get_radius=5000,  # Rayon r√©duit √† 5 km (au lieu de 25 km)
            pickable=True,
            auto_highlight=True,
            opacity=0.8,
        )
        
        # D√©finir la vue initiale (centr√©e sur l'Occitanie)
        view_state = pdk.ViewState(
            longitude=1.4442,  # Coordonn√©es approximatives pour l'Occitanie
            latitude=43.6047,
            zoom=7,
            pitch=0,
        )
        
        # Cr√©er le tooltip avec les valeurs format√©es
        tooltip = {
            "html": "<b>Station:</b> {nom_station}<br>",
            "style": {"backgroundColor": "steelblue", "color": "white"}
        }
        
        # Ajouter les informations de d√©bit si disponibles
        if 'debit_moyen_fmt' in map_data.columns:
            tooltip["html"] += "<b>D√©bit moyen:</b> {debit_moyen_fmt} m¬≥/s<br>"
        
        if 'seuil_alerte_fmt' in map_data.columns:
            tooltip["html"] += "<b>Seuil alerte:</b> {seuil_alerte_fmt} m¬≥/s"
        
        # Cr√©er la carte
        r = pdk.Deck(
            layers=[stations_layer],
            initial_view_state=view_state,
            tooltip=tooltip,
            map_style="light",
        )
        
        return r
    except Exception as e:
        st.error(f"Erreur lors de la cr√©ation de la carte: {str(e)}")
        return None

def main():
    # Titre de l'application
    st.title("üåä Eko - Surveillance des Rivi√®res d'Occitanie")
    
    # Charger les donn√©es
    data = load_hydro_data()
    
    # Si aucun fichier par d√©faut n'est trouv√©, permettre √† l'utilisateur d'uploader son propre fichier
    if data is None:
        st.warning("Aucun fichier de donn√©es par d√©faut trouv√©. Veuillez uploader votre CSV de donn√©es hydrom√©triques.")
        uploaded_file = st.file_uploader("Choisissez un fichier CSV", type="csv")
        if uploaded_file is not None:
            data = pd.read_csv(uploaded_file)
            # Convertir les dates et ajouter les colonnes temporelles
            date_column = 'date_obs_elab' if 'date_obs_elab' in data.columns else 'date_obs'
            if date_column in data.columns:
                data['date'] = pd.to_datetime(data[date_column])
                data['year'] = data['date'].dt.year
                data['month'] = data['date'].dt.month
                data['day'] = data['date'].dt.day
                data['day_of_year'] = data['date'].dt.dayofyear
    
    # V√©rifier si des donn√©es sont disponibles
    if data is None or len(data) == 0:
        st.error("Aucune donn√©e disponible. Veuillez v√©rifier votre fichier CSV.")
        return
    
    # Extraire les informations des stations
    stations_info = extract_stations_info(data)
    
    # Calcul des seuils
    with st.spinner("Calcul des seuils d'alerte..."):
        thresholds_df = calculate_thresholds(data)
        monthly_thresholds_df = calculate_monthly_thresholds(data)
    
    # Afficher des informations sur les donn√©es charg√©es
    st.sidebar.subheader("Informations sur les donn√©es")
    st.sidebar.info(f"Nombre de stations: {data['code_station_id'].nunique()}")
    st.sidebar.info(f"Nombre total d'observations: {len(data)}")
    
    if 'date' in data.columns:
        st.sidebar.info(f"P√©riode des donn√©es: du {data['date'].min().strftime('%d/%m/%Y')} au {data['date'].max().strftime('%d/%m/%Y')}")
    
    # Sidebar pour les contr√¥les
    st.sidebar.title("Contr√¥les")
    
    # 1. S√©lection de la station
    station_options = data[['code_station_id', 'nom_station']].drop_duplicates()
    
    # V√©rifier si station_options n'est pas vide
    if len(station_options) > 0:
        selected_station = st.sidebar.selectbox(
            "S√©lectionner une station",
            station_options['code_station_id'].tolist(),
            format_func=lambda x: f"{x} - {station_options[station_options['code_station_id'] == x]['nom_station'].values[0]}"
        )
    else:
        st.error("Aucune station disponible. V√©rifiez le format de vos donn√©es.")
        return
    
    # 2. Type d'affichage
    display_type = st.sidebar.radio(
    "Type d'affichage",
    ["Carte des stations", "Historique des d√©bits", "Seuils d'alerte", "Pr√©dictions", "M√©thodologie"])
    
    # 3. Options pour les pr√©dictions
    if display_type == "Pr√©dictions":
        prediction_days = st.sidebar.slider(
            "Nombre de jours de pr√©diction",
            min_value=7,
            max_value=90,
            value=30,
            step=1
        )
        
        prediction_method = st.sidebar.radio(
            "M√©thode de pr√©diction",
            ["RandomForest", "Prophet"],
            help="RandomForest est g√©n√©ralement plus pr√©cis pour ce type de donn√©es. Prophet est meilleur pour les tendances saisonni√®res."
        )
    else:
        prediction_days = 30  # Valeur par d√©faut
        prediction_method = "RandomForest"  # M√©thode par d√©faut
    
    # Affichage principal en fonction du type s√©lectionn√©
    if display_type == "Carte des stations":
        st.header("Carte des stations hydrom√©triques")
        
        # Colonne d'information
        st.markdown("""
        **L√©gende**:
        - üîµ Bleu: Station avec d√©bit normal
        - üü† Orange: Station au-dessus du seuil d'alerte
        - üî¥ Rouge: Station au-dessus du seuil de crise
        - ‚ö™ Gris: Station sans donn√©es de d√©bit
        """)
        
        # Afficher la carte
        map_chart = display_station_map(stations_info, thresholds_df)
        if map_chart:
            st.pydeck_chart(map_chart)
        
        # Informations suppl√©mentaires
        st.subheader("Informations sur les stations")
        
        # S√©lectionner les colonnes pertinentes pour l'affichage
        display_columns = ['code_station_id', 'nom_station', 'longitude', 'latitude']
        
        if stations_info is not None:
            st.dataframe(stations_info[display_columns])
    
    elif display_type == "Historique des d√©bits":
        st.header(f"Historique des d√©bits pour la station {selected_station}")
        
        # Filtrer les donn√©es pour la station s√©lectionn√©e
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in data.columns else 'resultat_obs'
        station_data = data[data['code_station_id'] == selected_station].copy()
        station_data = station_data.sort_values('date')
        
        # V√©rifier si des donn√©es sont disponibles pour cette station
        if len(station_data) == 0:
            st.warning(f"Aucune donn√©e de d√©bit disponible pour la station {selected_station}.")
            return
        
        # Obtenir les informations de la station
        station_name = station_options[station_options['code_station_id'] == selected_station]['nom_station'].iloc[0]
        st.subheader(f"Station: {station_name}")
        
        # Cr√©er un graphique des d√©bits avec Plotly
        fig = px.scatter(
            station_data,
            x='date',
            y=flow_column,
            title=f"D√©bits (m¬≥/s) - Station {selected_station}",
            labels={flow_column: "D√©bit (m¬≥/s)", 'date': 'Date'},
            template="plotly_white"
        )
        
        # Ajouter une ligne de tendance
        fig.add_traces(
            px.line(station_data, x='date', y=flow_column).data[0]
        )
        
        # Ajouter les seuils d'alerte s'ils existent
        if thresholds_df is not None:
            threshold_data = thresholds_df[thresholds_df['code_station_id'] == selected_station]
            
            if len(threshold_data) > 0:
                threshold_data = threshold_data.iloc[0]
                fig.add_hline(y=threshold_data['seuil_vigilance'], line_dash="dash", line_color="yellow", annotation_text="Vigilance")
                fig.add_hline(y=threshold_data['seuil_alerte'], line_dash="dash", line_color="orange", annotation_text="Alerte")
                fig.add_hline(y=threshold_data['seuil_alerte_renforcee'], line_dash="dash", line_color="orangered", annotation_text="Alerte renforc√©e")
                fig.add_hline(y=threshold_data['seuil_crise'], line_dash="dash", line_color="red", annotation_text="Crise")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Statistiques des d√©bits
        st.subheader("Statistiques des d√©bits")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("D√©bit minimum", f"{station_data[flow_column].min():.2f} m¬≥/s")
        col2.metric("D√©bit moyen", f"{station_data[flow_column].mean():.2f} m¬≥/s")
        col3.metric("D√©bit maximum", f"{station_data[flow_column].max():.2f} m¬≥/s")
        col4.metric("√âcart-type", f"{station_data[flow_column].std():.2f} m¬≥/s")
        
        # R√©partition saisonni√®re
        if 'saison' in station_data.columns:
            st.subheader("R√©partition saisonni√®re")
            
            # Statistiques par saison
            seasonal_stats = station_data.groupby('saison')[flow_column].agg(['mean', 'min', 'max']).reset_index()
            
            # Ordonner les saisons correctement
            season_order = ['Hiver', 'Printemps', '√ât√©', 'Automne']
            seasonal_stats['saison'] = pd.Categorical(seasonal_stats['saison'], categories=season_order, ordered=True)
            seasonal_stats = seasonal_stats.sort_values('saison')
            
            # Cr√©er un graphique des d√©bits par saison
            fig_season = px.bar(
                seasonal_stats,
                x='saison',
                y='mean',
                title="D√©bit moyen par saison",
                labels={'mean': "D√©bit moyen (m¬≥/s)", 'saison': 'Saison'},
                text_auto='.2f',
                color='saison',
                color_discrete_map={
                    'Hiver': 'royalblue',
                    'Printemps': 'mediumseagreen',
                    '√ât√©': 'orange',
                    'Automne': 'brown'
                }
            )
            
            st.plotly_chart(fig_season, use_container_width=True)
    
    elif display_type == "Seuils d'alerte":
        st.header("Seuils d'alerte calcul√©s")
        
        # 1. Seuils globaux
        st.subheader("Seuils globaux par station")
        if thresholds_df is not None:
            # Joindre les noms des stations pour une meilleure lisibilit√©
            display_thresholds = pd.merge(
                thresholds_df,
                station_options,
                on='code_station_id',
                how='left'
            )
            
            # Formater les colonnes num√©riques pour un affichage plus lisible
            for col in display_thresholds.columns:
                if col not in ['code_station_id', 'nom_station'] and pd.api.types.is_numeric_dtype(display_thresholds[col]):
                    display_thresholds[col] = display_thresholds[col].round(2)
            
            st.dataframe(display_thresholds)
        else:
            st.warning("Aucun seuil global calcul√©.")
        
        # 2. Seuils mensuels pour la station s√©lectionn√©e
        st.subheader(f"Seuils mensuels pour la station {selected_station}")
        
        if monthly_thresholds_df is not None:
            monthly_data = monthly_thresholds_df[monthly_thresholds_df['code_station_id'] == selected_station].copy()
            
            if len(monthly_data) == 0:
                st.warning(f"Aucune donn√©e mensuelle disponible pour la station {selected_station}.")
            else:
                # Ajouter les noms des mois
                month_names = {
                    1: 'Janvier', 2: 'F√©vrier', 3: 'Mars', 4: 'Avril', 5: 'Mai', 6: 'Juin',
                    7: 'Juillet', 8: 'Ao√ªt', 9: 'Septembre', 10: 'Octobre', 11: 'Novembre', 12: 'D√©cembre'
                }
                monthly_data['nom_mois'] = monthly_data['month'].map(month_names)
                
                # Trier par mois
                monthly_data = monthly_data.sort_values('month')
                
                # Formater les valeurs num√©riques
                for col in monthly_data.columns:
                    if col not in ['code_station_id', 'month', 'nom_mois'] and pd.api.types.is_numeric_dtype(monthly_data[col]):
                        monthly_data[col] = monthly_data[col].round(2)
                
                # Afficher le tableau des seuils mensuels
                st.dataframe(monthly_data[['nom_mois', 'debit_moyen', 'seuil_vigilance', 'seuil_alerte', 'seuil_alerte_renforcee', 'seuil_crise']])
                
                # Visualisation des seuils mensuels
                fig = px.line(
                    monthly_data, 
                    x='month', 
                    y=['debit_moyen', 'seuil_vigilance', 'seuil_alerte', 'seuil_alerte_renforcee', 'seuil_crise'],
                    title=f"Variation mensuelle des seuils pour la station {selected_station}",
                    labels={'month': 'Mois', 'value': 'D√©bit (m¬≥/s)', 'variable': 'Type de seuil'},
                    color_discrete_map={
                        'debit_moyen': 'blue',
                        'seuil_vigilance': 'yellow',
                        'seuil_alerte': 'orange',
                        'seuil_alerte_renforcee': 'orangered',
                        'seuil_crise': 'red'
                    }
                )
                
                # Am√©liorer l'axe des x pour afficher les noms des mois
                fig.update_xaxes(
                    tickvals=list(range(1, 13)),
                    ticktext=list(month_names.values())
                )
                
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("Aucun seuil mensuel calcul√©.")
    
    elif display_type == "Pr√©dictions":
        st.header(f"Pr√©dictions de d√©bit pour la station {selected_station}")
        
        # Obtenir le nom de la station
        station_name = station_options[station_options['code_station_id'] == selected_station]['nom_station'].iloc[0]
        
        # Filtrer les donn√©es pour la station s√©lectionn√©e
        flow_column = 'resultat_obs_elab' if 'resultat_obs_elab' in data.columns else 'resultat_obs'
        station_data = data[data['code_station_id'] == selected_station].copy()
        
        if len(station_data) < 10:
            st.warning("Donn√©es insuffisantes pour faire des pr√©dictions fiables. Un minimum de 10 points de donn√©es est recommand√©.")
            if len(station_data) == 0:
                return
        
        # Onglets pour s√©parer les pr√©dictions futures et l'√©valuation
        pred_tab, eval_tab = st.tabs(["Pr√©dictions futures", "√âvaluation du mod√®le"])
        
        with pred_tab:
            # Cr√©er et entra√Æner le mod√®le
            with st.spinner("Entra√Ænement du mod√®le de pr√©diction..."):
                try:
                    model_info = create_prediction_model(data, selected_station, method=prediction_method.lower())
                    if model_info is None:
                        st.error("Impossible de cr√©er le mod√®le de pr√©diction. V√©rifiez les donn√©es d'entr√©e.")
                        return
                    
                    forecast = make_predictions(model_info, data, selected_station, days=prediction_days)
                    if forecast is None:
                        st.error("√âchec de la pr√©diction. V√©rifiez le mod√®le.")
                        return
                except Exception as e:
                    st.error(f"Erreur lors de la cr√©ation du mod√®le de pr√©diction: {str(e)}")
                    st.exception(e)  # Afficher la trace compl√®te
                    return
            
            # Visualiser les pr√©dictions
            fig = go.Figure()
            
            # Donn√©es historiques
            fig.add_trace(go.Scatter(
                x=station_data['date'],
                y=station_data[flow_column],
                mode='markers',
                name='Donn√©es historiques',
                marker=dict(color='blue', size=6)
            ))
            
            # Ajouter une ligne de tendance pour les donn√©es historiques
            fig.add_trace(go.Scatter(
                x=station_data['date'],
                y=station_data[flow_column],
                mode='lines',
                name='Tendance historique',
                line=dict(color='royalblue', width=1)
            ))
            
            # Pr√©dictions
            fig.add_trace(go.Scatter(
                x=forecast['date'],
                y=forecast['prediction'],
                mode='lines',
                name='Pr√©diction',
                line=dict(color='red', width=2)
            ))
            
            # Intervalle de confiance
            fig.add_trace(go.Scatter(
                x=forecast['date'],
                y=forecast['upper_bound'],
                mode='lines',
                line=dict(width=0),
                showlegend=False
            ))
            
            fig.add_trace(go.Scatter(
                x=forecast['date'],
                y=forecast['lower_bound'],
                mode='lines',
                line=dict(width=0),
                fill='tonexty',
                fillcolor='rgba(255, 0, 0, 0.2)',
                name='Intervalle de confiance'
            ))
            
            # Ajouter les seuils d'alerte s'ils existent
            if thresholds_df is not None:
                threshold_data = thresholds_df[thresholds_df['code_station_id'] == selected_station]
                
                if len(threshold_data) > 0:
                    threshold_data = threshold_data.iloc[0]
                    fig.add_hline(y=threshold_data['seuil_vigilance'], line_dash="dash", line_color="yellow", annotation_text="Vigilance")
                    fig.add_hline(y=threshold_data['seuil_alerte'], line_dash="dash", line_color="orange", annotation_text="Alerte")
                    fig.add_hline(y=threshold_data['seuil_crise'], line_dash="dash", line_color="red", annotation_text="Crise")
            
            # Mise en forme du graphique
            fig.update_layout(
                title=f"Pr√©diction des d√©bits pour {station_name}",
                xaxis_title="Date",
                yaxis_title="D√©bit (m¬≥/s)",
                hovermode="x unified",
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                )
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Informations sur les d√©passements de seuil pr√©vus
            if thresholds_df is not None:
                threshold_data = thresholds_df[thresholds_df['code_station_id'] == selected_station]
                
                if len(threshold_data) > 0:
                    threshold_data = threshold_data.iloc[0]
                    
                    try:
                        # V√©rifier s'il y a des d√©passements pr√©vus
                        alert_days = forecast[forecast['prediction'] > threshold_data['seuil_alerte']]
                        crisis_days = forecast[forecast['prediction'] > threshold_data['seuil_crise']]
                        
                        st.subheader("Analyse des risques")
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.metric(
                                "Jours de d√©passement du seuil d'alerte pr√©vus", 
                                len(alert_days),
                                delta=None,
                                delta_color="inverse"
                            )
                            
                            if len(alert_days) > 0:
                                st.write("Dates pr√©vues de d√©passement du seuil d'alerte:")
                                alert_dates = [d.strftime("%d/%m/%Y") for d in alert_days['date']]
                                # Limiter le nombre de dates affich√©es si n√©cessaire
                                if len(alert_dates) > 10:
                                    st.write(", ".join(alert_dates[:10]) + f" et {len(alert_dates) - 10} autres jours")
                                else:
                                    st.write(", ".join(alert_dates))
                        
                        with col2:
                            st.metric(
                                "Jours de d√©passement du seuil de crise pr√©vus", 
                                len(crisis_days),
                                delta=None,
                                delta_color="inverse"
                            )
                            
                            if len(crisis_days) > 0:
                                st.write("Dates pr√©vues de d√©passement du seuil de crise:")
                                crisis_dates = [d.strftime("%d/%m/%Y") for d in crisis_days['date']]
                                # Limiter le nombre de dates affich√©es si n√©cessaire
                                if len(crisis_dates) > 10:
                                    st.write(", ".join(crisis_dates[:10]) + f" et {len(crisis_dates) - 10} autres jours")
                                else:
                                    st.write(", ".join(crisis_dates))
                    except Exception as e:
                        st.error(f"Erreur lors de l'analyse des risques: {str(e)}")
        
        with eval_tab:
            st.subheader("√âvaluation de la performance du mod√®le")
            
            # √âvaluer le mod√®le
            with st.spinner("√âvaluation du mod√®le en cours..."):
                evaluation = evaluate_model(model_info, data, selected_station)
                
                if evaluation["error"]:
                    st.error(evaluation["error"])
                elif evaluation["metrics"] is None:
                    st.warning("Impossible de calculer les m√©triques d'√©valuation.")
                else:
                    # Afficher les m√©triques dans une mise en page propre
                    st.markdown("### M√©triques de performance")
                    
                    # Utiliser des colonnes pour afficher les m√©triques c√¥te √† c√¥te
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric("MAE (Erreur absolue moyenne)", f"{evaluation['metrics']['MAE']:.2f} m¬≥/s")
                        st.metric("R¬≤ (Coefficient de d√©termination)", f"{evaluation['metrics']['R¬≤']:.3f}")
                        
                    with col2:
                        st.metric("RMSE (Racine de l'erreur quadratique moyenne)", f"{evaluation['metrics']['RMSE']:.2f} m¬≥/s")
                        st.metric("MAPE (% d'erreur absolue moyen)", f"{evaluation['metrics']['MAPE']:.2f}%")
                    
                    # Afficher l'interpr√©tation des r√©sultats
                    st.markdown("""
                    **Interpr√©tation des m√©triques:**
                    - **MAE**: Indique l'erreur moyenne en valeur absolue. Plus la valeur est basse, meilleur est le mod√®le.
                    - **RMSE**: Plus sensible aux erreurs importantes. Une valeur plus basse est meilleure.
                    - **R¬≤**: Varie de 0 √† 1. Plus la valeur est proche de 1, mieux le mod√®le explique la variance des donn√©es.
                    - **MAPE**: Exprime l'erreur en pourcentage, permettant une interpr√©tation plus intuitive.
                    """)
                    
                    # Cr√©er un graphique pour comparer les pr√©dictions et les valeurs r√©elles
                    fig = go.Figure()
                    
                    # Ajouter les valeurs r√©elles
                    fig.add_trace(go.Scatter(
                        x=evaluation["dates"],
                        y=evaluation["actuals"],
                        mode='markers',
                        name='Valeurs r√©elles',
                        marker=dict(color='blue', size=8)
                    ))
                    
                    # Ajouter les pr√©dictions
                    fig.add_trace(go.Scatter(
                        x=evaluation["dates"],
                        y=evaluation["predictions"],
                        mode='lines+markers',
                        name='Pr√©dictions',
                        line=dict(color='red', width=2),
                        marker=dict(color='red', size=6)
                    ))
                    
                    # Mise en forme du graphique
                    fig.update_layout(
                        title=f"Comparaison des pr√©dictions et valeurs r√©elles ({model_info['type']})",
                        xaxis_title="Date",
                        yaxis_title="D√©bit (m¬≥/s)",
                        hovermode="x unified"
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Afficher un r√©sum√© de l'√©valuation
                    st.markdown(f"""
                    ### R√©sum√© de l'√©valuation
                    
                    Le mod√®le **{model_info['type']}** a √©t√© √©valu√© sur {len(evaluation['actuals'])} points de donn√©es de test.
                    
                    **Performance g√©n√©rale**: 
                    {'Bonne' if evaluation['metrics']['R¬≤'] > 0.7 else 'Moyenne' if evaluation['metrics']['R¬≤'] > 0.5 else 'Faible'}
                    
                    - Le mod√®le pr√©sente une erreur moyenne de **{evaluation['metrics']['MAE']:.2f} m¬≥/s**
                    - En termes relatifs, l'erreur moyenne est de **{evaluation['metrics']['MAPE']:.2f}%**
                    - Le mod√®le explique **{evaluation['metrics']['R¬≤']*100:.1f}%** de la variance des donn√©es
                    """)
    
    elif display_type == "M√©thodologie":
        st.header("M√©thodologie et explications d√©taill√©es")
        
        # Utilisation des onglets pour organiser les explications
        tab1, tab2, tab3, tab4 = st.tabs(["Sources de donn√©es", "Calcul des seuils", "Mod√®les de pr√©diction", "Limites et perspectives"])
        
        with tab1:
            st.subheader("Sources et collecte des donn√©es")
            st.markdown("""
            ### Origine des donn√©es
            
            Les donn√©es hydrom√©triques utilis√©es dans cette application proviennent de l'**API Hub'Eau** (Hydrom√©trie) qui centralise 
            les mesures des stations de surveillance des cours d'eau fran√ßais.
            
            ### Processus de collecte
            
            1. **S√©lection des stations** : L'application utilise les donn√©es de stations hydrom√©triques situ√©es en r√©gion Occitanie.
            - Nombre total de stations surveill√©es : {stations_count}
            - P√©riode couverte : {period_start} √† {period_end}
            
            2. **Types de donn√©es collect√©es** :
            - **D√©bits journaliers** (QmnJ) : Valeurs moyennes journali√®res des d√©bits en m¬≥/s
            - **Hauteurs d'eau** (HmnJ) : Mesures des niveaux d'eau en m√®tres
            
            3. **Volume de donn√©es** :
            - Nombre total d'observations : {total_obs}
            - R√©partition temporelle : Les donn√©es sont r√©parties sur plusieurs saisons pour assurer une bonne repr√©sentativit√©
            """.format(
                stations_count=data['code_station_id'].nunique(),
                period_start=data['date'].min().strftime('%d/%m/%Y'),
                period_end=data['date'].max().strftime('%d/%m/%Y'),
                total_obs=len(data)
            ))
            
            # Ajouter un graphique montrant la r√©partition des donn√©es
            if 'saison' in data.columns:
                season_counts = data['saison'].value_counts().reset_index()
                season_counts.columns = ['Saison', 'Nombre d\'observations']
                
                # Cr√©er un ordre personnalis√© pour les saisons
                season_order = ['Hiver', 'Printemps', '√ât√©', 'Automne']
                season_counts['Saison'] = pd.Categorical(season_counts['Saison'], categories=season_order, ordered=True)
                season_counts = season_counts.sort_values('Saison')
                
                fig = px.bar(
                    season_counts,
                    x='Saison',
                    y='Nombre d\'observations',
                    title="R√©partition des observations par saison",
                    color='Saison',
                    color_discrete_map={
                        'Hiver': 'royalblue',
                        'Printemps': 'mediumseagreen',
                        '√ât√©': 'orange',
                        'Automne': 'brown'
                    }
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.subheader("Calcul des seuils d'alerte")
            st.markdown("""
            ### M√©thodologie de calcul des seuils
            
            Les seuils d'alerte sont calcul√©s √† partir des statistiques historiques des d√©bits pour chaque station. L'application 
            utilise une approche bas√©e sur les percentiles de la distribution des d√©bits observ√©s.
            
            **Seuils calcul√©s** :
            
            1. **Seuil de vigilance** (75√®me percentile)
            - Repr√©sente le niveau au-del√† duquel une attention particuli√®re est recommand√©e
            - Formule : 75% des observations historiques sont inf√©rieures √† ce seuil
            
            2. **Seuil d'alerte** (90√®me percentile)
            - Indique un niveau √©lev√© n√©cessitant une surveillance accrue
            - Formule : 90% des observations historiques sont inf√©rieures √† ce seuil
            
            3. **Seuil d'alerte renforc√©e** (95√®me percentile)
            - Signale un niveau tr√®s √©lev√© pouvant conduire √† des restrictions
            - Formule : 95% des observations historiques sont inf√©rieures √† ce seuil
            
            4. **Seuil de crise** (98√®me percentile)
            - Indique un niveau critique requ√©rant des mesures d'urgence
            - Formule : 98% des observations historiques sont inf√©rieures √† ce seuil
            
            ### Seuils mensuels
            
            L'application calcule √©galement des seuils sp√©cifiques pour chaque mois de l'ann√©e, permettant de prendre en compte 
            les variations saisonni√®res des d√©bits. Cette approche permet une plus grande pr√©cision dans la d√©tection des anomalies.
            
            **Note importante** : Ces seuils sont calcul√©s √† titre indicatif √† partir des donn√©es historiques et ne remplacent pas 
            les seuils r√©glementaires officiels d√©finis par les autorit√©s comp√©tentes.
            """)
            
            # Illustration du calcul des percentiles
            st.image("https://miro.medium.com/max/1200/1*2c21SkzJMf3frPXPAR_gZA.png", 
                    caption="Illustration de la notion de percentile sur une distribution statistique")
        
        with tab3:
            st.subheader("Mod√®les de pr√©diction")
            st.markdown("""
            ### M√©thodes de pr√©diction utilis√©es
            
            L'application propose deux approches diff√©rentes pour la pr√©diction des d√©bits futurs :
            
            #### 1. Random Forest (For√™t al√©atoire)
            
            Un mod√®le d'apprentissage automatique bas√© sur un ensemble d'arbres de d√©cision.
            
            **Caract√©ristiques** :
            - Utilise les variables : mois, jour de l'ann√©e, saison
            - Nombre d'arbres : 100
            - Validation crois√©e : S√©paration 80% entra√Ænement / 20% test
            
            **Avantages** :
            - Capture les relations non-lin√©aires
            - Robuste face aux valeurs aberrantes
            - Performant avec peu de donn√©es
            
            #### 2. Prophet (d√©velopp√© par Facebook)
            
            Un mod√®le de s√©rie temporelle con√ßu pour capturer les tendances et saisonnalit√©s.
            
            **Caract√©ristiques** :
            - D√©compose la s√©rie en tendance, saisonnalit√© et composante r√©siduelle
            - Int√®gre des facteurs de saisonnalit√© quotidienne
            
            **Avantages** :
            - Capture efficacement les tendances et cycles saisonniers
            - G√®re bien les donn√©es manquantes
            - Fournit des intervalles de confiance
            
            ### Intervalles de confiance
            
            Les pr√©dictions sont accompagn√©es d'intervalles de confiance pour indiquer le niveau d'incertitude.
            
            - Pour **Random Forest** : Intervalle calcul√© √† partir de l'√©cart-type (¬± 1.96œÉ)
            - Pour **Prophet** : Intervalles de pr√©diction natifs
            """)
            
            # Diagramme illustrant le processus de pr√©diction
            st.markdown("""
            ### Processus de pr√©diction
            
            ```
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Donn√©es       ‚îÇ     ‚îÇ  Entra√Ænement  ‚îÇ     ‚îÇ  G√©n√©ration    ‚îÇ
            ‚îÇ  historiques   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  du mod√®le     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  de pr√©dictions ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                      ‚îÇ                      ‚îÇ
                    ‚îÇ                      ‚îÇ                      ‚îÇ
                    ‚ñº                      ‚ñº                      ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Extraction    ‚îÇ     ‚îÇ  Validation    ‚îÇ     ‚îÇ  Analyse des   ‚îÇ
            ‚îÇ  des variables ‚îÇ     ‚îÇ  crois√©e       ‚îÇ     ‚îÇ  risques       ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ```
            """)
            
            # Ajouter des m√©triques d'√©valuation
            st.markdown("""
            ### √âvaluation des mod√®les
            
            Pour √©valuer la qualit√© des pr√©dictions, l'application utilise plusieurs m√©triques :
            
            1. **MAE (Mean Absolute Error)** : Erreur absolue moyenne entre les pr√©dictions et les valeurs r√©elles
            2. **RMSE (Root Mean Square Error)** : Racine carr√©e de l'erreur quadratique moyenne
            3. **R¬≤ (Coefficient de d√©termination)** : Mesure la proportion de la variance expliqu√©e par le mod√®le
            4. **MAPE (Mean Absolute Percentage Error)** : Mesure l'erreur en pourcentage pour une meilleure interpr√©tation
            
            Ces m√©triques sont calcul√©es sur un ensemble de test (20% des donn√©es les plus r√©centes) qui n'a pas √©t√© utilis√© 
            pour l'entra√Ænement du mod√®le, assurant ainsi une √©valuation impartiale.
            """)
        
        with tab4:
            st.subheader("Limites et perspectives d'am√©lioration")
            st.markdown("""
            ### Limites actuelles
            
            **Limites des donn√©es** :
            - Certaines stations disposent de peu d'observations
            - Donn√©es parfois discontinues dans le temps
            - Absence de certaines variables contextuelles (pr√©cipitations, temp√©rature)
            
            **Limites des mod√®les** :
            - Pr√©dictions √† plus de 30 jours peuvent manquer de fiabilit√©
            - Difficult√© √† pr√©voir des √©v√©nements extr√™mes
            - Mod√®les non calibr√©s pour les crues exceptionnelles
            
            ### Perspectives d'am√©lioration
            
            **Enrichissement des donn√©es** :
            - Int√©gration de donn√©es m√©t√©orologiques (pr√©cipitations, temp√©ratures)
            - Ajout de caract√©ristiques g√©ographiques des bassins versants
            - Collecte de donn√©es historiques plus compl√®tes
            
            **Am√©liorations techniques** :
            - D√©veloppement de mod√®les hybrides combinant diff√©rentes approches
            - Utilisation de techniques d'apprentissage profond pour les s√©ries temporelles
            - Int√©gration d'algorithmes de d√©tection d'anomalies plus sophistiqu√©s
            
            **√âvolutions fonctionnelles** :
            - Syst√®me d'alerte par email ou SMS
            - Interface mobile optimis√©e
            - Int√©gration avec d'autres sources de donn√©es environnementales
            """)
            
            # Ajouter une citation ou r√©f√©rence
            st.info("""
            **Pour en savoir plus** : Cette application s'inspire des m√©thodes utilis√©es par le Service Central 
            d'Hydrom√©t√©orologie et d'Appui √† la Pr√©vision des Inondations (SCHAPI) et les Services de Pr√©vision des Crues (SPC).
            """)
    
    # Pied de page
    st.markdown("---")
    st.markdown("### √Ä propos")
    st.markdown("""
    Cette application permet de visualiser et d'analyser les donn√©es des stations hydrom√©triques d'Occitanie. 
    Elle calcule des seuils d'alerte bas√©s sur les statistiques historiques et propose un mod√®le de pr√©diction des d√©bits.
    
    **Note:** Les seuils calcul√©s sont bas√©s uniquement sur l'analyse statistique des donn√©es historiques et ne remplacent pas les seuils r√©glementaires officiels.
    """)

if __name__ == "__main__":
    main()
